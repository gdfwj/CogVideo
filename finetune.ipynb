{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first download sample dataset from Disney-VideoGeneration-Dataset and store it in /data/zihao/Disney-VideoGeneration-Dataset\n",
    "\n",
    "then use the first frame of videos as images to test i2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not need to run again\n",
    "# import os\n",
    "# import cv2\n",
    "\n",
    "# dataset_path = \"/data/zihao/Disney-VideoGeneration-Dataset\"\n",
    "# videos_txt_path = os.path.join(dataset_path, \"videos.txt\")\n",
    "# images_dir = os.path.join(dataset_path, \"images\")\n",
    "# images_txt_path = os.path.join(dataset_path, \"images.txt\")\n",
    "\n",
    "# # make images folder\n",
    "# os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "# # 读取 videos.txt\n",
    "# with open(videos_txt_path, \"r\") as f:\n",
    "#     video_paths = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# image_paths = []\n",
    "\n",
    "# for video_path in video_paths:\n",
    "#     video_full_path = os.path.join(dataset_path, video_path)\n",
    "    \n",
    "#     cap = cv2.VideoCapture(video_full_path)\n",
    "#     success, frame = cap.read()\n",
    "#     cap.release()\n",
    "    \n",
    "#     if success:\n",
    "#         image_name = os.path.splitext(os.path.basename(video_path))[0] + \".jpg\"\n",
    "#         image_path = os.path.join(images_dir, image_name)\n",
    "#         cv2.imwrite(image_path, frame)\n",
    "#         image_paths.append(os.path.relpath(image_path, dataset_path))\n",
    "#     else:\n",
    "#         print(f\"failed in {video_path}\")\n",
    "\n",
    "\n",
    "# with open(images_txt_path, \"w\") as f:\n",
    "#     for image_path in image_paths:\n",
    "#         f.write(image_path + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup model configurations (same as the train_ddp_i2v.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Args(model_path=PosixPath('THUDM/CogVideoX-5B-I2V'), model_name='cogvideox-i2v', model_type='i2v', training_type='lora', output_dir=PosixPath('finetune_output'), report_to='tensorboard', tracker_name='finetrainer-cogvideo', data_root=PosixPath('/data/zihao/one_data'), caption_column=PosixPath('prompt.txt'), image_column=PosixPath('images.txt'), video_column=PosixPath('videos.txt'), resume_from_checkpoint=None, seed=42, train_epochs=1000, train_steps=None, checkpointing_steps=10, checkpointing_limit=2, batch_size=1, gradient_accumulation_steps=1, train_resolution=(49, 480, 720), mixed_precision='bf16', learning_rate=2e-05, optimizer='adamw', beta1=0.9, beta2=0.95, beta3=0.98, epsilon=1e-08, weight_decay=0.0001, max_grad_norm=1.0, lr_scheduler='constant_with_warmup', lr_warmup_steps=100, lr_num_cycles=1, lr_power=1.0, num_workers=8, pin_memory=True, gradient_checkpointing=True, enable_slicing=False, enable_tiling=True, nccl_timeout=1800, rank=128, lora_alpha=64, target_modules=['to_q', 'to_k', 'to_v', 'to_out.0'], do_validation=False, validation_steps=20, validation_dir=PosixPath('/data/zihao/one_data'), validation_prompts='prompts.txt', validation_images='images.txt', validation_videos='', gen_fps=16)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "%load_ext autoreload\n",
    "from finetune.schemas import Args\n",
    "def get_training_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Training arguments for CogVideoX-5B-I2V\")\n",
    "    \n",
    "    # Model Configuration\n",
    "    parser.add_argument(\"--model_path\", type=str, default=\"THUDM/CogVideoX-5B-I2V\")\n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"cogvideox-i2v\")\n",
    "    parser.add_argument(\"--model_type\", type=str, default=\"i2v\")\n",
    "    parser.add_argument(\"--training_type\", type=str, default=\"lora\")\n",
    "    \n",
    "    # Output Configuration\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"finetune_output\")\n",
    "    parser.add_argument(\"--report_to\", type=str, default=\"tensorboard\")\n",
    "    \n",
    "    # Data Configuration\n",
    "    parser.add_argument(\"--data_root\", type=str, default=\"/data/zihao/one_data\")\n",
    "    parser.add_argument(\"--caption_column\", type=str, default=\"prompt.txt\")\n",
    "    parser.add_argument(\"--video_column\", type=str, default=\"videos.txt\")\n",
    "    parser.add_argument(\"--image_column\", type=str, default=\"images.txt\")  # Empty by default\n",
    "    parser.add_argument(\"--train_resolution\", type=str, default=\"49x480x720\")\n",
    "    \n",
    "    # Training Configuration\n",
    "    parser.add_argument(\"--train_epochs\", type=int, default=1000)\n",
    "    parser.add_argument(\"--seed\", type=int, default=42)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1)\n",
    "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1)\n",
    "    parser.add_argument(\"--mixed_precision\", type=str, default=\"bf16\")\n",
    "    \n",
    "    # System Configuration\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=8)\n",
    "    parser.add_argument(\"--pin_memory\", type=bool, default=True)\n",
    "    parser.add_argument(\"--nccl_timeout\", type=int, default=1800)\n",
    "    \n",
    "    # Checkpointing Configuration\n",
    "    parser.add_argument(\"--checkpointing_steps\", type=int, default=10)\n",
    "    parser.add_argument(\"--checkpointing_limit\", type=int, default=2)\n",
    "    # parser.add_argument(\"--resume_from_checkpoint\", type=str, default=\"/home/zihao/CogVideo/finetune/finetune_output/checkpoint-70\")\n",
    "    \n",
    "    # Validation Configuration\n",
    "    parser.add_argument(\"--do_validation\", type=bool, default=False)\n",
    "    parser.add_argument(\"--validation_dir\", type=str, default=\"/data/zihao/one_data\")\n",
    "    parser.add_argument(\"--validation_steps\", type=int, default=20)\n",
    "    parser.add_argument(\"--validation_prompts\", type=str, default=\"prompts.txt\")\n",
    "    parser.add_argument(\"--validation_images\", type=str, default=\"images.txt\")\n",
    "    parser.add_argument(\"--validation_videos\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--gen_fps\", type=int, default=16)\n",
    "    \n",
    "    parser.add_argument(\"--enable_slicing\", type=bool, default=False)\n",
    "    \n",
    "    return parser.parse_args([]) \n",
    "args = get_training_args()\n",
    "args.train_resolution = [int(x) for x in args.train_resolution.split('x')]\n",
    "arg_dict = vars(args)\n",
    "args = Args(**arg_dict)\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zihao/miniconda3/envs/cog/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/zihao/miniconda3/envs/cog/lib/python3.12/site-packages/transformers/utils/hub.py:106: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 4094.00it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.81s/it]\n",
      "Fetching 3 files: 100%|██████████| 3/3 [00:00<00:00, 28468.13it/s]\n",
      "03/03/2025 06:29:56 - INFO - trainer - Initialized Trainer\n",
      "03/03/2025 06:29:56 - INFO - trainer - Accelerator state: \n",
      "Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: bf16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from finetune.models.utils import get_model_cls\n",
    "from finetune.schemas import Args\n",
    "import torch\n",
    "os.environ[\"WANDB_PROJECT\"] = \"DDPO_distill\"\n",
    "os.environ[\"WANDB_ENTITY\"] = \"pandora_distill\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "trainer_cls = get_model_cls(args.model_name, args.training_type)\n",
    "trainer = trainer_cls(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/03/2025 06:29:59 - INFO - trainer - Initializing models\n",
      "03/03/2025 06:29:59 - INFO - trainer - Initializing dataset and dataloader\n",
      "03/03/2025 06:30:07 - INFO - trainer - Precomputing latent for video and prompt embedding ...\n",
      "03/03/2025 06:30:08 - INFO - trainer - Precomputing latent for video and prompt embedding ... Done\n",
      "03/03/2025 06:30:15 - INFO - trainer - Initializing trainable parameters\n",
      "03/03/2025 06:30:19 - INFO - trainer - Initializing optimizer and lr scheduler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'default': LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'to_q', 'to_k', 'to_out.0', 'to_v'}, exclude_modules=None, lora_alpha=64, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, eva_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/03/2025 06:30:41 - INFO - trainer - Initializing trackers\n",
      "03/03/2025 06:30:41 - INFO - trainer - Starting training\n",
      "03/03/2025 06:30:41 - INFO - trainer - Memory before training start: {\n",
      "    \"memory_allocated\": 22.077,\n",
      "    \"memory_reserved\": 22.088,\n",
      "    \"max_memory_allocated\": 22.077,\n",
      "    \"max_memory_reserved\": 22.088\n",
      "}\n",
      "03/03/2025 06:30:41 - INFO - trainer - Training configuration: {\n",
      "    \"trainable parameters\": 132120576,\n",
      "    \"total samples\": 1,\n",
      "    \"train epochs\": 1000,\n",
      "    \"train steps\": 1000,\n",
      "    \"batches per device\": 1,\n",
      "    \"total batches observed per epoch\": 1,\n",
      "    \"train batch size total count\": 1,\n",
      "    \"gradient accumulation steps\": 1\n",
      "}\n",
      "Training steps:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-03 06:30:46,024] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/03/2025 06:30:46 - INFO - root - gcc -pthread -B /home/zihao/miniconda3/envs/cog/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/zihao/miniconda3/envs/cog/include -fPIC -O2 -isystem /home/zihao/miniconda3/envs/cog/include -fPIC -c /tmp/tmpk8bdeha3/test.c -o /tmp/tmpk8bdeha3/test.o\n",
      "03/03/2025 06:30:46 - INFO - root - gcc -pthread -B /home/zihao/miniconda3/envs/cog/compiler_compat /tmp/tmpk8bdeha3/test.o -laio -o /tmp/tmpk8bdeha3/a.out\n",
      "/home/zihao/miniconda3/envs/cog/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "03/03/2025 06:30:46 - INFO - root - gcc -pthread -B /home/zihao/miniconda3/envs/cog/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/zihao/miniconda3/envs/cog/include -fPIC -O2 -isystem /home/zihao/miniconda3/envs/cog/include -fPIC -c /tmp/tmp6q7ahhgt/test.c -o /tmp/tmp6q7ahhgt/test.o\n",
      "03/03/2025 06:30:46 - INFO - root - gcc -pthread -B /home/zihao/miniconda3/envs/cog/compiler_compat /tmp/tmp6q7ahhgt/test.o -L/usr -L/usr/lib64 -lcufile -o /tmp/tmp6q7ahhgt/a.out\n",
      "/home/zihao/miniconda3/envs/cog/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CogVideo/finetune/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_validation()\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_trackers()\n\u001b[0;32m--> 650\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CogVideo/finetune/trainer.py:426\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    422\u001b[0m logs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m accelerator\u001b[38;5;241m.\u001b[39maccumulate(models_to_accumulate):\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;66;03m# These weighting schemes use a uniform timestep sampling and instead post-weight the loss\u001b[39;00m\n\u001b[0;32m--> 426\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m     accelerator\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accelerator\u001b[38;5;241m.\u001b[39msync_gradients:\n",
      "File \u001b[0;32m~/CogVideo/finetune/models/cogvideox_i2v/lora_trainer.py:209\u001b[0m, in \u001b[0;36mCogVideoXI2VLoraTrainer.compute_loss\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# 使用 pipeline_with_logprob 生成图像\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 209\u001b[0m     images, _, latents, log_probs \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_with_logprob\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnegative_prompt_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 根据需要设置\u001b[39;49;00m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 根据需要设置\u001b[39;49;00m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 根据需要设置\u001b[39;49;00m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 根据需要设置\u001b[39;49;00m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m latents \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(latents, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size, num_steps + 1, 4, 64, 64)\u001b[39;00m\n\u001b[1;32m    220\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(log_probs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size, num_steps, 1)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference through finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files: 100%|██████████| 3/3 [00:00<00:00, 27294.82it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m lora_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinetune/finetune_output/checkpoint-690\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(lora_model_path))\n\u001b[0;32m---> 14\u001b[0m transformer \u001b[38;5;241m=\u001b[39m \u001b[43mCogVideoXTransformer3DModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransformer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m text_encoder \u001b[38;5;241m=\u001b[39m T5EncoderModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_encoder\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m vae \u001b[38;5;241m=\u001b[39m AutoencoderKLCogVideoX\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvae\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/cog/lib/python3.12/site-packages/diffusers/models/modeling_utils.py:1077\u001b[0m, in \u001b[0;36mModelMixin.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m is_bitsandbytes_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.43.2\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1073\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1074\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling `to()` is not supported for `4-bit` quantized models with the installed version of bitsandbytes. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1075\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe current device is `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. If you intended to move the model, please install bitsandbytes >= 0.43.2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1076\u001b[0m         )\n\u001b[0;32m-> 1077\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cog/lib/python3.12/site-packages/torch/nn/modules/module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cog/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cog/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cog/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cog/lib/python3.12/site-packages/torch/nn/modules/module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1324\u001b[0m             device,\n\u001b[1;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1326\u001b[0m             non_blocking,\n\u001b[1;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1328\u001b[0m         )\n\u001b[0;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import AutoencoderKLCogVideoX, CogVideoXImageToVideoPipeline, CogVideoXTransformer3DModel\n",
    "from diffusers.utils import export_to_video, load_image\n",
    "from transformers import T5EncoderModel\n",
    "import os\n",
    "from peft import PeftModel\n",
    "model_id = \"THUDM/CogVideoX-5b-I2V\"\n",
    "torch.cuda.set_device(1)\n",
    "device = \"cuda:1\"\n",
    "lora_model_path = \"finetune/finetune_output/checkpoint-690\"\n",
    "\n",
    "print(os.path.exists(lora_model_path))\n",
    "\n",
    "transformer = CogVideoXTransformer3DModel.from_pretrained(model_id, subfolder=\"transformer\", torch_dtype=torch.float16).to(device)\n",
    "\n",
    "text_encoder = T5EncoderModel.from_pretrained(model_id, subfolder=\"text_encoder\", torch_dtype=torch.float16).to(device)\n",
    "vae = AutoencoderKLCogVideoX.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float16).to(device)\n",
    "\n",
    "pipe = CogVideoXImageToVideoPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    text_encoder=text_encoder,\n",
    "    transformer=transformer,\n",
    "    vae=vae,\n",
    "    torch_dtype=torch.float16,\n",
    ").to(device)\n",
    "\n",
    "pipe.load_lora_weights(lora_model_path, weight_name=\"pytorch_lora_weights.safetensors\", adapter_name=\"test_1\")\n",
    "pipe.fuse_lora(components=[\"transformer\"], lora_scale=1 / 128)\n",
    "\n",
    "pipe.enable_sequential_cpu_offload()\n",
    "\n",
    "prompt = \"A man rides a horse along a dusty trail, surrounded by a vast desert landscape. The sun sets on the horizon, casting a warm golden glow across the sky, with scattered clouds adding texture to the scene. Tall cacti stand like sentinels on either side of the path, their silhouettes stark against the fading light. As the horse trots steadily forward, the rider takes in the serene beauty of the open wilderness, the gentle breeze rustling through the sparse vegetation. Birds can be seen flying in the distance, adding a sense of tranquility to the moment.\"\n",
    "image = load_image(\"/data/zihao/one_data/images/image.jpg\")\n",
    "\n",
    "video = pipe(image=image, prompt=prompt, guidance_scale=6, use_dynamic_cfg=True, num_inference_steps=50).frames[0]\n",
    "\n",
    "export_to_video(video, \"output-finetuned.mp4\", fps=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
