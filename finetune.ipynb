{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first download sample dataset from Disney-VideoGeneration-Dataset and store it in /data/zihao/Disney-VideoGeneration-Dataset\n",
    "\n",
    "then use the first frame of videos as images to test i2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not need to run again\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "dataset_path = \"/data/zihao/Disney-VideoGeneration-Dataset\"\n",
    "videos_txt_path = os.path.join(dataset_path, \"videos.txt\")\n",
    "images_dir = os.path.join(dataset_path, \"images\")\n",
    "images_txt_path = os.path.join(dataset_path, \"images.txt\")\n",
    "\n",
    "# make images folder\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "# 读取 videos.txt\n",
    "with open(videos_txt_path, \"r\") as f:\n",
    "    video_paths = [line.strip() for line in f.readlines()]\n",
    "\n",
    "image_paths = []\n",
    "\n",
    "for video_path in video_paths:\n",
    "    video_full_path = os.path.join(dataset_path, video_path)\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_full_path)\n",
    "    success, frame = cap.read()\n",
    "    cap.release()\n",
    "    \n",
    "    if success:\n",
    "        image_name = os.path.splitext(os.path.basename(video_path))[0] + \".jpg\"\n",
    "        image_path = os.path.join(images_dir, image_name)\n",
    "        cv2.imwrite(image_path, frame)\n",
    "        image_paths.append(os.path.relpath(image_path, dataset_path))\n",
    "    else:\n",
    "        print(f\"failed in {video_path}\")\n",
    "\n",
    "\n",
    "with open(images_txt_path, \"w\") as f:\n",
    "    for image_path in image_paths:\n",
    "        f.write(image_path + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup model configurations (same as the train_ddp_i2v.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Args(model_path=PosixPath('THUDM/CogVideoX-5B-I2V'), model_name='cogvideox-i2v', model_type='i2v', training_type='lora', output_dir=PosixPath('finetune_output'), report_to='tensorboard', tracker_name='finetrainer-cogvideo', data_root=PosixPath('/data/zihao/Disney-VideoGeneration-Dataset'), caption_column=PosixPath('prompt.txt'), image_column=PosixPath('images.txt'), video_column=PosixPath('videos.txt'), resume_from_checkpoint=PosixPath('/home/zihao/CogVideo/finetune/finetune_output/checkpoint-70'), seed=42, train_epochs=10, train_steps=None, checkpointing_steps=10, checkpointing_limit=2, batch_size=1, gradient_accumulation_steps=1, train_resolution=(49, 480, 720), mixed_precision='bf16', learning_rate=2e-05, optimizer='adamw', beta1=0.9, beta2=0.95, beta3=0.98, epsilon=1e-08, weight_decay=0.0001, max_grad_norm=1.0, lr_scheduler='constant_with_warmup', lr_warmup_steps=100, lr_num_cycles=1, lr_power=1.0, num_workers=8, pin_memory=True, gradient_checkpointing=True, enable_slicing=False, enable_tiling=True, nccl_timeout=1800, rank=128, lora_alpha=64, target_modules=['to_q', 'to_k', 'to_v', 'to_out.0'], do_validation=False, validation_steps=20, validation_dir=PosixPath('/data/zihao/Disney-VideoGeneration-Dataset'), validation_prompts='prompts.txt', validation_images='images.txt', validation_videos='', gen_fps=16)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "%load_ext autoreload\n",
    "from finetune.schemas import Args\n",
    "def get_training_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Training arguments for CogVideoX-5B-I2V\")\n",
    "    \n",
    "    # Model Configuration\n",
    "    parser.add_argument(\"--model_path\", type=str, default=\"THUDM/CogVideoX-5B-I2V\")\n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"cogvideox-i2v\")\n",
    "    parser.add_argument(\"--model_type\", type=str, default=\"i2v\")\n",
    "    parser.add_argument(\"--training_type\", type=str, default=\"lora\")\n",
    "    \n",
    "    # Output Configuration\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"finetune_output\")\n",
    "    parser.add_argument(\"--report_to\", type=str, default=\"tensorboard\")\n",
    "    \n",
    "    # Data Configuration\n",
    "    parser.add_argument(\"--data_root\", type=str, default=\"/data/zihao/Disney-VideoGeneration-Dataset\")\n",
    "    parser.add_argument(\"--caption_column\", type=str, default=\"prompt.txt\")\n",
    "    parser.add_argument(\"--video_column\", type=str, default=\"videos.txt\")\n",
    "    parser.add_argument(\"--image_column\", type=str, default=\"images.txt\")  # Empty by default\n",
    "    parser.add_argument(\"--train_resolution\", type=str, default=\"49x480x720\")\n",
    "    \n",
    "    # Training Configuration\n",
    "    parser.add_argument(\"--train_epochs\", type=int, default=10)\n",
    "    parser.add_argument(\"--seed\", type=int, default=42)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1)\n",
    "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1)\n",
    "    parser.add_argument(\"--mixed_precision\", type=str, default=\"bf16\")\n",
    "    \n",
    "    # System Configuration\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=8)\n",
    "    parser.add_argument(\"--pin_memory\", type=bool, default=True)\n",
    "    parser.add_argument(\"--nccl_timeout\", type=int, default=1800)\n",
    "    \n",
    "    # Checkpointing Configuration\n",
    "    parser.add_argument(\"--checkpointing_steps\", type=int, default=10)\n",
    "    parser.add_argument(\"--checkpointing_limit\", type=int, default=2)\n",
    "    parser.add_argument(\"--resume_from_checkpoint\", type=str, default=\"/home/zihao/CogVideo/finetune/finetune_output/checkpoint-70\")\n",
    "    \n",
    "    # Validation Configuration\n",
    "    parser.add_argument(\"--do_validation\", type=bool, default=False)\n",
    "    parser.add_argument(\"--validation_dir\", type=str, default=\"/data/zihao/Disney-VideoGeneration-Dataset\")\n",
    "    parser.add_argument(\"--validation_steps\", type=int, default=20)\n",
    "    parser.add_argument(\"--validation_prompts\", type=str, default=\"prompts.txt\")\n",
    "    parser.add_argument(\"--validation_images\", type=str, default=\"images.txt\")\n",
    "    parser.add_argument(\"--validation_videos\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--gen_fps\", type=int, default=16)\n",
    "    \n",
    "    parser.add_argument(\"--enable_slicing\", type=bool, default=False)\n",
    "    \n",
    "    return parser.parse_args([]) \n",
    "args = get_training_args()\n",
    "args.train_resolution = [int(x) for x in args.train_resolution.split('x')]\n",
    "arg_dict = vars(args)\n",
    "args = Args(**arg_dict)\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zihao/miniconda3/envs/cogvideo/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/zihao/miniconda3/envs/cogvideo/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 4330.72it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it]\n",
      "Fetching 3 files: 100%|██████████| 3/3 [00:00<00:00, 22919.69it/s]\n",
      "02/15/2025 10:24:50 - INFO - trainer - Initialized Trainer\n",
      "02/15/2025 10:24:50 - INFO - trainer - Accelerator state: \n",
      "Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: bf16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from finetune.models.utils import get_model_cls\n",
    "from finetune.schemas import Args\n",
    "import torch\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "trainer_cls = get_model_cls(args.model_name, args.training_type)\n",
    "trainer = trainer_cls(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/15/2025 10:24:50 - INFO - trainer - Initializing models\n",
      "02/15/2025 10:24:50 - INFO - trainer - Initializing dataset and dataloader\n",
      "02/15/2025 10:24:56 - INFO - trainer - Precomputing latent for video and prompt embedding ...\n",
      "02/15/2025 10:25:00 - INFO - trainer - Precomputing latent for video and prompt embedding ... Done\n",
      "02/15/2025 10:25:08 - INFO - trainer - Initializing trainable parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'default': LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'to_v', 'to_out.0', 'to_k', 'to_q'}, exclude_modules=None, lora_alpha=64, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, eva_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/15/2025 10:25:12 - INFO - trainer - Initializing optimizer and lr scheduler\n",
      "02/15/2025 10:25:32 - INFO - trainer - Initializing trackers\n",
      "02/15/2025 10:25:32 - INFO - trainer - Starting training\n",
      "02/15/2025 10:25:32 - INFO - trainer - Memory before training start: {\n",
      "    \"memory_allocated\": 22.077,\n",
      "    \"memory_reserved\": 22.088,\n",
      "    \"max_memory_allocated\": 22.077,\n",
      "    \"max_memory_reserved\": 22.088\n",
      "}\n",
      "02/15/2025 10:25:32 - INFO - trainer - Training configuration: {\n",
      "    \"trainable parameters\": 132120576,\n",
      "    \"total samples\": 69,\n",
      "    \"train epochs\": 10,\n",
      "    \"train steps\": 690,\n",
      "    \"batches per device\": 1,\n",
      "    \"total batches observed per epoch\": 69,\n",
      "    \"train batch size total count\": 1,\n",
      "    \"gradient accumulation steps\": 1\n",
      "}\n",
      "02/15/2025 10:25:32 - INFO - trainer - Resuming from checkpoint /home/zihao/CogVideo/finetune/finetune_output/checkpoint-70\n",
      "02/15/2025 10:25:32 - INFO - accelerate.accelerator - Loading states from /home/zihao/CogVideo/finetune/finetune_output/checkpoint-70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-15 10:25:32,836] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/15/2025 10:25:32 - INFO - root - gcc -pthread -B /home/zihao/miniconda3/envs/cogvideo/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/zihao/miniconda3/envs/cogvideo/include -fPIC -O2 -isystem /home/zihao/miniconda3/envs/cogvideo/include -fPIC -c /tmp/tmpmux1depa/test.c -o /tmp/tmpmux1depa/test.o\n",
      "02/15/2025 10:25:32 - INFO - root - gcc -pthread -B /home/zihao/miniconda3/envs/cogvideo/compiler_compat /tmp/tmpmux1depa/test.o -laio -o /tmp/tmpmux1depa/a.out\n",
      "/home/zihao/miniconda3/envs/cogvideo/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "02/15/2025 10:25:33 - INFO - root - gcc -pthread -B /home/zihao/miniconda3/envs/cogvideo/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/zihao/miniconda3/envs/cogvideo/include -fPIC -O2 -isystem /home/zihao/miniconda3/envs/cogvideo/include -fPIC -c /tmp/tmpma6lz2qz/test.c -o /tmp/tmpma6lz2qz/test.o\n",
      "02/15/2025 10:25:33 - INFO - root - gcc -pthread -B /home/zihao/miniconda3/envs/cogvideo/compiler_compat /tmp/tmpma6lz2qz/test.o -L/usr -L/usr/lib64 -lcufile -o /tmp/tmpma6lz2qz/a.out\n",
      "/home/zihao/miniconda3/envs/cogvideo/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "02/15/2025 10:25:33 - INFO - accelerate.checkpointing - All model weights loaded successfully\n",
      "02/15/2025 10:25:34 - INFO - accelerate.checkpointing - All optimizer states loaded successfully\n",
      "02/15/2025 10:25:34 - INFO - accelerate.checkpointing - All scheduler states loaded successfully\n",
      "02/15/2025 10:25:34 - INFO - accelerate.checkpointing - All dataloader sampler states loaded successfully\n",
      "02/15/2025 10:25:34 - INFO - accelerate.checkpointing - All random states loaded successfully\n",
      "02/15/2025 10:25:34 - INFO - accelerate.accelerator - Loading in 0 custom states\n",
      "Training steps:  10%|█         | 70/690 [00:00<?, ?it/s]Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fcb9a6ca710>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zihao/miniconda3/envs/cogvideo/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "Training steps:  10%|█         | 71/690 [00:11<1:56:46, 11.32s/it, grad_norm=0.00444, loss=0.0828, lr=2e-5]"
     ]
    }
   ],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference through finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files: 100%|██████████| 3/3 [00:00<00:00, 28086.86it/s]\n",
      "Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 4629.47it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.94s/it]\n",
      "Loading pipeline components...: 100%|██████████| 5/5 [00:00<00:00, 19.21it/s]\n",
      "100%|██████████| 50/50 [06:05<00:00,  7.31s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output-finetuned.mp4'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import AutoencoderKLCogVideoX, CogVideoXImageToVideoPipeline, CogVideoXTransformer3DModel\n",
    "from diffusers.utils import export_to_video, load_image\n",
    "from transformers import T5EncoderModel\n",
    "import os\n",
    "from peft import PeftModel\n",
    "model_id = \"THUDM/CogVideoX-5b-I2V\"\n",
    "torch.cuda.set_device(7)\n",
    "device = \"cuda:7\"\n",
    "lora_model_path = \"finetune_output/checkpoint-690\"\n",
    "\n",
    "print(os.path.exists(lora_model_path))\n",
    "\n",
    "transformer = CogVideoXTransformer3DModel.from_pretrained(model_id, subfolder=\"transformer\", torch_dtype=torch.float16).to(device)\n",
    "\n",
    "text_encoder = T5EncoderModel.from_pretrained(model_id, subfolder=\"text_encoder\", torch_dtype=torch.float16).to(device)\n",
    "vae = AutoencoderKLCogVideoX.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float16).to(device)\n",
    "\n",
    "pipe = CogVideoXImageToVideoPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    text_encoder=text_encoder,\n",
    "    transformer=transformer,\n",
    "    vae=vae,\n",
    "    torch_dtype=torch.float16,\n",
    ").to(device)\n",
    "\n",
    "pipe.load_lora_weights(lora_model_path, weight_name=\"pytorch_lora_weights.safetensors\", adapter_name=\"test_1\")\n",
    "pipe.fuse_lora(components=[\"transformer\"], lora_scale=1 / 128)\n",
    "\n",
    "pipe.enable_sequential_cpu_offload()\n",
    "\n",
    "prompt = \"An astronaut hatching from an egg, on the surface of the moon, the darkness and depth of space realised in the background. High quality, ultrarealistic detail and breath-taking movie-like camera shot.\"\n",
    "image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/astronaut.jpg\")\n",
    "\n",
    "video = pipe(image=image, prompt=prompt, guidance_scale=6, use_dynamic_cfg=True, num_inference_steps=50).frames[0]\n",
    "\n",
    "export_to_video(video, \"output-finetuned.mp4\", fps=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogvideo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
