{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first download sample dataset from Disney-VideoGeneration-Dataset and store it in /data/zihao/Disney-VideoGeneration-Dataset\n",
    "\n",
    "then use the first frame of videos as images to test i2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not need to run again\n",
    "# import os\n",
    "# import cv2\n",
    "\n",
    "# dataset_path = \"/data/zihao/Disney-VideoGeneration-Dataset\"\n",
    "# videos_txt_path = os.path.join(dataset_path, \"videos.txt\")\n",
    "# images_dir = os.path.join(dataset_path, \"images\")\n",
    "# images_txt_path = os.path.join(dataset_path, \"images.txt\")\n",
    "\n",
    "# # make images folder\n",
    "# os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "# # 读取 videos.txt\n",
    "# with open(videos_txt_path, \"r\") as f:\n",
    "#     video_paths = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# image_paths = []\n",
    "\n",
    "# for video_path in video_paths:\n",
    "#     video_full_path = os.path.join(dataset_path, video_path)\n",
    "    \n",
    "#     cap = cv2.VideoCapture(video_full_path)\n",
    "#     success, frame = cap.read()\n",
    "#     cap.release()\n",
    "    \n",
    "#     if success:\n",
    "#         image_name = os.path.splitext(os.path.basename(video_path))[0] + \".jpg\"\n",
    "#         image_path = os.path.join(images_dir, image_name)\n",
    "#         cv2.imwrite(image_path, frame)\n",
    "#         image_paths.append(os.path.relpath(image_path, dataset_path))\n",
    "#     else:\n",
    "#         print(f\"failed in {video_path}\")\n",
    "\n",
    "\n",
    "# with open(images_txt_path, \"w\") as f:\n",
    "#     for image_path in image_paths:\n",
    "#         f.write(image_path + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup model configurations (same as the train_ddp_i2v.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Args(model_path=PosixPath('THUDM/CogVideoX-5B-I2V'), model_name='cogvideox-i2v', model_type='i2v', training_type='lora', output_dir=PosixPath('finetune_output'), report_to='tensorboard', tracker_name='finetrainer-cogvideo', data_root=PosixPath('/data/zihao/one_data'), caption_column=PosixPath('prompt.txt'), image_column=PosixPath('images.txt'), video_column=PosixPath('videos.txt'), resume_from_checkpoint=None, seed=42, train_epochs=1000, train_steps=None, checkpointing_steps=10, checkpointing_limit=2, batch_size=1, gradient_accumulation_steps=1, train_resolution=(49, 480, 720), mixed_precision='bf16', learning_rate=2e-05, optimizer='adamw', beta1=0.9, beta2=0.95, beta3=0.98, epsilon=1e-08, weight_decay=0.0001, max_grad_norm=1.0, lr_scheduler='constant_with_warmup', lr_warmup_steps=100, lr_num_cycles=1, lr_power=1.0, num_workers=8, pin_memory=True, gradient_checkpointing=True, enable_slicing=False, enable_tiling=True, nccl_timeout=1800, rank=128, lora_alpha=64, target_modules=['to_q', 'to_k', 'to_v', 'to_out.0'], do_validation=False, validation_steps=20, validation_dir=PosixPath('/data/zihao/one_data'), validation_prompts='prompts.txt', validation_images='images.txt', validation_videos='', gen_fps=16)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "%load_ext autoreload\n",
    "from finetune.schemas import Args\n",
    "def get_training_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Training arguments for CogVideoX-5B-I2V\")\n",
    "    \n",
    "    # Model Configuration\n",
    "    parser.add_argument(\"--model_path\", type=str, default=\"THUDM/CogVideoX-5B-I2V\")\n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"cogvideox-i2v\")\n",
    "    parser.add_argument(\"--model_type\", type=str, default=\"i2v\")\n",
    "    parser.add_argument(\"--training_type\", type=str, default=\"lora\")\n",
    "    \n",
    "    # Output Configuration\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"finetune_output\")\n",
    "    parser.add_argument(\"--report_to\", type=str, default=\"tensorboard\")\n",
    "    \n",
    "    # Data Configuration\n",
    "    parser.add_argument(\"--data_root\", type=str, default=\"/data/zihao/one_data\")\n",
    "    parser.add_argument(\"--caption_column\", type=str, default=\"prompt.txt\")\n",
    "    parser.add_argument(\"--video_column\", type=str, default=\"videos.txt\")\n",
    "    parser.add_argument(\"--image_column\", type=str, default=\"images.txt\")  # Empty by default\n",
    "    parser.add_argument(\"--train_resolution\", type=str, default=\"49x480x720\")\n",
    "    \n",
    "    # Training Configuration\n",
    "    parser.add_argument(\"--train_epochs\", type=int, default=1000)\n",
    "    parser.add_argument(\"--seed\", type=int, default=42)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1)\n",
    "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1)\n",
    "    parser.add_argument(\"--mixed_precision\", type=str, default=\"bf16\")\n",
    "    \n",
    "    # System Configuration\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=8)\n",
    "    parser.add_argument(\"--pin_memory\", type=bool, default=True)\n",
    "    parser.add_argument(\"--nccl_timeout\", type=int, default=1800)\n",
    "    \n",
    "    # Checkpointing Configuration\n",
    "    parser.add_argument(\"--checkpointing_steps\", type=int, default=10)\n",
    "    parser.add_argument(\"--checkpointing_limit\", type=int, default=2)\n",
    "    # parser.add_argument(\"--resume_from_checkpoint\", type=str, default=\"/home/zihao/CogVideo/finetune/finetune_output/checkpoint-70\")\n",
    "    \n",
    "    # Validation Configuration\n",
    "    parser.add_argument(\"--do_validation\", type=bool, default=False)\n",
    "    parser.add_argument(\"--validation_dir\", type=str, default=\"/data/zihao/one_data\")\n",
    "    parser.add_argument(\"--validation_steps\", type=int, default=20)\n",
    "    parser.add_argument(\"--validation_prompts\", type=str, default=\"prompts.txt\")\n",
    "    parser.add_argument(\"--validation_images\", type=str, default=\"images.txt\")\n",
    "    parser.add_argument(\"--validation_videos\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--gen_fps\", type=int, default=16)\n",
    "    \n",
    "    parser.add_argument(\"--enable_slicing\", type=bool, default=False)\n",
    "    \n",
    "    return parser.parse_args([]) \n",
    "args = get_training_args()\n",
    "args.train_resolution = [int(x) for x in args.train_resolution.split('x')]\n",
    "arg_dict = vars(args)\n",
    "args = Args(**arg_dict)\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zihao/miniconda3/envs/cog/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/zihao/miniconda3/envs/cog/lib/python3.12/site-packages/transformers/utils/hub.py:106: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 4094.00it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.81s/it]\n",
      "Fetching 3 files: 100%|██████████| 3/3 [00:00<00:00, 28468.13it/s]\n",
      "03/03/2025 06:29:56 - INFO - trainer - Initialized Trainer\n",
      "03/03/2025 06:29:56 - INFO - trainer - Accelerator state: \n",
      "Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: bf16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from finetune.models.utils import get_model_cls\n",
    "from finetune.schemas import Args\n",
    "import torch\n",
    "os.environ[\"WANDB_PROJECT\"] = \"DDPO_distill\"\n",
    "os.environ[\"WANDB_ENTITY\"] = \"pandora_distill\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "trainer_cls = get_model_cls(args.model_name, args.training_type)\n",
    "trainer = trainer_cls(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/03/2025 06:29:59 - INFO - trainer - Initializing models\n",
      "03/03/2025 06:29:59 - INFO - trainer - Initializing dataset and dataloader\n",
      "03/03/2025 06:30:07 - INFO - trainer - Precomputing latent for video and prompt embedding ...\n",
      "03/03/2025 06:30:08 - INFO - trainer - Precomputing latent for video and prompt embedding ... Done\n",
      "03/03/2025 06:30:15 - INFO - trainer - Initializing trainable parameters\n",
      "03/03/2025 06:30:19 - INFO - trainer - Initializing optimizer and lr scheduler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'default': LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'to_q', 'to_k', 'to_out.0', 'to_v'}, exclude_modules=None, lora_alpha=64, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, eva_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/03/2025 06:30:41 - INFO - trainer - Initializing trackers\n",
      "03/03/2025 06:30:41 - INFO - trainer - Starting training\n",
      "03/03/2025 06:30:41 - INFO - trainer - Memory before training start: {\n",
      "    \"memory_allocated\": 22.077,\n",
      "    \"memory_reserved\": 22.088,\n",
      "    \"max_memory_allocated\": 22.077,\n",
      "    \"max_memory_reserved\": 22.088\n",
      "}\n",
      "03/03/2025 06:30:41 - INFO - trainer - Training configuration: {\n",
      "    \"trainable parameters\": 132120576,\n",
      "    \"total samples\": 1,\n",
      "    \"train epochs\": 1000,\n",
      "    \"train steps\": 1000,\n",
      "    \"batches per device\": 1,\n",
      "    \"total batches observed per epoch\": 1,\n",
      "    \"train batch size total count\": 1,\n",
      "    \"gradient accumulation steps\": 1\n",
      "}\n",
      "Training steps:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-03 06:30:46,024] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/03/2025 06:30:46 - INFO - root - gcc -pthread -B /home/zihao/miniconda3/envs/cog/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/zihao/miniconda3/envs/cog/include -fPIC -O2 -isystem /home/zihao/miniconda3/envs/cog/include -fPIC -c /tmp/tmpk8bdeha3/test.c -o /tmp/tmpk8bdeha3/test.o\n",
      "03/03/2025 06:30:46 - INFO - root - gcc -pthread -B /home/zihao/miniconda3/envs/cog/compiler_compat /tmp/tmpk8bdeha3/test.o -laio -o /tmp/tmpk8bdeha3/a.out\n",
      "/home/zihao/miniconda3/envs/cog/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "03/03/2025 06:30:46 - INFO - root - gcc -pthread -B /home/zihao/miniconda3/envs/cog/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/zihao/miniconda3/envs/cog/include -fPIC -O2 -isystem /home/zihao/miniconda3/envs/cog/include -fPIC -c /tmp/tmp6q7ahhgt/test.c -o /tmp/tmp6q7ahhgt/test.o\n",
      "03/03/2025 06:30:46 - INFO - root - gcc -pthread -B /home/zihao/miniconda3/envs/cog/compiler_compat /tmp/tmp6q7ahhgt/test.o -L/usr -L/usr/lib64 -lcufile -o /tmp/tmp6q7ahhgt/a.out\n",
      "/home/zihao/miniconda3/envs/cog/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CogVideo/finetune/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_validation()\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_trackers()\n\u001b[0;32m--> 650\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CogVideo/finetune/trainer.py:426\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    422\u001b[0m logs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m accelerator\u001b[38;5;241m.\u001b[39maccumulate(models_to_accumulate):\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;66;03m# These weighting schemes use a uniform timestep sampling and instead post-weight the loss\u001b[39;00m\n\u001b[0;32m--> 426\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m     accelerator\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accelerator\u001b[38;5;241m.\u001b[39msync_gradients:\n",
      "File \u001b[0;32m~/CogVideo/finetune/models/cogvideox_i2v/lora_trainer.py:209\u001b[0m, in \u001b[0;36mCogVideoXI2VLoraTrainer.compute_loss\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# 使用 pipeline_with_logprob 生成图像\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 209\u001b[0m     images, _, latents, log_probs \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_with_logprob\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnegative_prompt_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 根据需要设置\u001b[39;49;00m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 根据需要设置\u001b[39;49;00m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 根据需要设置\u001b[39;49;00m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 根据需要设置\u001b[39;49;00m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m latents \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(latents, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size, num_steps + 1, 4, 64, 64)\u001b[39;00m\n\u001b[1;32m    220\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(log_probs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size, num_steps, 1)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference through finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zihao/miniconda3/envs/cog/lib/python3.12/site-packages/transformers/utils/hub.py:106: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Fetching 3 files: 100%|██████████| 3/3 [00:00<00:00, 23519.46it/s]\n",
      "Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 4522.16it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]\n",
      "Loading pipeline components...: 100%|██████████| 5/5 [00:00<00:00, 20.20it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_input:  torch.Size([1, 13, 16, 60, 90])\n",
      "latent_input:  torch.Size([1, 13, 32, 60, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:07<01:08,  7.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_input:  torch.Size([1, 13, 16, 60, 90])\n",
      "latent_input:  torch.Size([1, 13, 32, 60, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:13<00:53,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_input:  torch.Size([1, 13, 16, 60, 90])\n",
      "latent_input:  torch.Size([1, 13, 32, 60, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:18<00:41,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_input:  torch.Size([1, 13, 16, 60, 90])\n",
      "latent_input:  torch.Size([1, 13, 32, 60, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:24<00:36,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_input:  torch.Size([1, 13, 16, 60, 90])\n",
      "latent_input:  torch.Size([1, 13, 32, 60, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:30<00:30,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_input:  torch.Size([1, 13, 16, 60, 90])\n",
      "latent_input:  torch.Size([1, 13, 32, 60, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:36<00:23,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_input:  torch.Size([1, 13, 16, 60, 90])\n",
      "latent_input:  torch.Size([1, 13, 32, 60, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:42<00:17,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_input:  torch.Size([1, 13, 16, 60, 90])\n",
      "latent_input:  torch.Size([1, 13, 32, 60, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:48<00:11,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_input:  torch.Size([1, 13, 16, 60, 90])\n",
      "latent_input:  torch.Size([1, 13, 32, 60, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:53<00:05,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_input:  torch.Size([1, 13, 16, 60, 90])\n",
      "latent_input:  torch.Size([1, 13, 32, 60, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:59<00:00,  5.97s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output-i2v10.mp4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import AutoencoderKLCogVideoX, CogVideoXImageToVideoPipeline, CogVideoXTransformer3DModel\n",
    "from diffusers.utils import export_to_video, load_image\n",
    "from transformers import T5EncoderModel\n",
    "import os\n",
    "from peft import PeftModel\n",
    "model_id = \"THUDM/CogVideoX-5B-I2V\"\n",
    "torch.cuda.set_device(1)\n",
    "device = \"cuda:1\"\n",
    "\n",
    "\n",
    "transformer = CogVideoXTransformer3DModel.from_pretrained(model_id, subfolder=\"transformer\", torch_dtype=torch.float16).to(device)\n",
    "\n",
    "text_encoder = T5EncoderModel.from_pretrained(model_id, subfolder=\"text_encoder\", torch_dtype=torch.float16).to(device)\n",
    "vae = AutoencoderKLCogVideoX.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float16).to(device)\n",
    "\n",
    "pipe = CogVideoXImageToVideoPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    text_encoder=text_encoder,\n",
    "    transformer=transformer,\n",
    "    vae=vae,\n",
    "    torch_dtype=torch.float16,\n",
    ").to(device)\n",
    "\n",
    "# lora_model_path = \"finetune/finetune_output/checkpoint-330\"\n",
    "\n",
    "# print(os.path.exists(lora_model_path))\n",
    "# pipe.load_lora_weights(lora_model_path, weight_name=\"pytorch_lora_weights.safetensors\", adapter_name=\"test_1\")\n",
    "# pipe.fuse_lora(components=[\"transformer\"], lora_scale=1 / 128)\n",
    "\n",
    "\n",
    "pipe.enable_sequential_cpu_offload()\n",
    "\n",
    "# with torch.no_grad():\n",
    "prompt = \"A man rides a horse along a dusty trail, surrounded by a vast desert landscape. The sun sets on the horizon, casting a warm golden glow across the sky, with scattered clouds adding texture to the scene. Tall cacti stand like sentinels on either side of the path, their silhouettes stark against the fading light. As the horse trots steadily forward, the rider takes in the serene beauty of the open wilderness, the gentle breeze rustling through the sparse vegetation. Birds can be seen flying in the distance, adding a sense of tranquility to the moment.\"\n",
    "image = load_image(\"/data/zihao/one_data/images/image.jpg\")\n",
    "\n",
    "video = pipe(prompt=prompt, image=image, guidance_scale=1, use_dynamic_cfg=False, num_inference_steps=10).frames[0]\n",
    "# print(len(video), video[0].shape)\n",
    "\n",
    "export_to_video(video, \"output-i2v10.mp4\", fps=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
